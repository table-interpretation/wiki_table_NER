{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3fcf22bc",
   "metadata": {},
   "source": [
    "### In this notebook, we itterate over the extracted tables, we transform their format and we add the span-based labels for the linked entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "86411217",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import csv\n",
    "import os\n",
    "import numpy as np\n",
    "import re\n",
    "import torch\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6dbce195",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0bdbfcda",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_nr = 42\n",
    "generator = torch.Generator().manual_seed(seed_nr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8d0fe4ff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "408781"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# These are the labeled entities which we extract in notebook 2.\n",
    "dataset_entities = pd.read_csv(\"../data/labeled_entities/full_dataset_entities_labeled_dbp_yago_final.csv\")\n",
    "dataset_entities.drop('Unnamed: 0', axis=1, inplace=True)\n",
    "len(dataset_entities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a9d2cb63",
   "metadata": {},
   "outputs": [],
   "source": [
    "entity_index = {entity: index for index, entity in dataset_entities['entity'].items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "64302d2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_mention(ent_text, tab_id, text_lookup, dataset_entities, entity_idex):\n",
    "    # For every entity, check if already added. If yes, we append the table id and the entity mention.\n",
    "    \n",
    "    row_index = entity_idex[text_lookup]\n",
    "    \n",
    "    current_mention = dataset_entities.at[row_index, 'mention']\n",
    "    current_table_id = dataset_entities.at[row_index, 'table_id']\n",
    "\n",
    "    # Update mention if different and non-null\n",
    "    if pd.isna(current_mention):\n",
    "        new_mention = ent_text\n",
    "    else:\n",
    "        new_mention = current_mention if ent_text == current_mention else f\"{current_mention},{ent_text}\" if ent_text else current_mention\n",
    "\n",
    "    # Update table_id if different and non-null\n",
    "    if pd.isna(current_table_id):\n",
    "        new_table_id = tab_id\n",
    "    else:\n",
    "        new_table_id = current_table_id if tab_id == current_table_id else f\"{current_table_id},{tab_id}\" if tab_id else current_table_id\n",
    "\n",
    "    # Assign the new values back to the DataFrame\n",
    "    dataset_entities.at[row_index, 'mention'] = new_mention\n",
    "    dataset_entities.at[row_index, 'table_id'] = new_table_id\n",
    "        \n",
    "    return dataset_entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "226f616c",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_dict = {\"None\":0,\n",
    "\"Activity\": 1,\n",
    "\"Organisation\": 2,\n",
    "\"ArchitecturalStructure\":3,\n",
    "\"Event\":4,\n",
    "\"Place\":5,\n",
    "\"Person\":6,\n",
    "\"Work\":7,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "08de65eb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Activity', 'Organisation', 'ArchitecturalStructure', 'Event',\n",
       "       'Place', 'Person', 'Work'], dtype=object)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_entities['class'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "34c96dbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "nclass = dataset_entities['class'].to_numpy()\n",
    "nwikidata_id = dataset_entities['wikidata_id'].to_numpy()\n",
    "nentity = dataset_entities['entity'].to_numpy()\n",
    "\n",
    "nlabels= list(zip(nclass,nwikidata_id))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c17133f9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('ArchitecturalStructure', 'Q2399023')"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# example for the lookup for the entity\n",
    "np.where(nentity == \"Germany\")[0]\n",
    "nlabels[41937]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1bf45cb2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "62433"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load the extracted tables\n",
    "data_path = \"../data/wiki_tabner_original_clean.json\"\n",
    "dataset_df = pd.read_json(data_path,lines=True)\n",
    "len(dataset_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "df62dd4e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>_id</th>\n",
       "      <th>numCols</th>\n",
       "      <th>numDataRows</th>\n",
       "      <th>numHeaderRows</th>\n",
       "      <th>numericColumns</th>\n",
       "      <th>order</th>\n",
       "      <th>pgId</th>\n",
       "      <th>pgTitle</th>\n",
       "      <th>sectionTitle</th>\n",
       "      <th>tableCaption</th>\n",
       "      <th>tableData</th>\n",
       "      <th>tableHeaders</th>\n",
       "      <th>tableId</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10003473-2</td>\n",
       "      <td>5</td>\n",
       "      <td>25</td>\n",
       "      <td>1</td>\n",
       "      <td>[0]</td>\n",
       "      <td>0.120435</td>\n",
       "      <td>10003473</td>\n",
       "      <td>Memphis Tigers men's basketball</td>\n",
       "      <td>NCAA Tournament Results</td>\n",
       "      <td>NCAA Tournament Results</td>\n",
       "      <td>[[{'cellID': -1, 'textTokens': [], 'text': '19...</td>\n",
       "      <td>[[{'cellID': -1, 'textTokens': [], 'text': 'Ye...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>100040-4</td>\n",
       "      <td>3</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>[]</td>\n",
       "      <td>0.680097</td>\n",
       "      <td>100040</td>\n",
       "      <td>Richmond Football Club</td>\n",
       "      <td>\"100 Tiger Treasures\"</td>\n",
       "      <td>\"100 Tiger Treasures\"</td>\n",
       "      <td>[[{'cellID': -1, 'textTokens': [], 'text': 'Be...</td>\n",
       "      <td>[[{'cellID': -1, 'textTokens': [], 'text': 'Aw...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          _id  numCols  numDataRows  numHeaderRows numericColumns     order  \\\n",
       "0  10003473-2        5           25              1            [0]  0.120435   \n",
       "1    100040-4        3           10              1             []  0.680097   \n",
       "\n",
       "       pgId                          pgTitle             sectionTitle  \\\n",
       "0  10003473  Memphis Tigers men's basketball  NCAA Tournament Results   \n",
       "1    100040           Richmond Football Club    \"100 Tiger Treasures\"   \n",
       "\n",
       "              tableCaption                                          tableData  \\\n",
       "0  NCAA Tournament Results  [[{'cellID': -1, 'textTokens': [], 'text': '19...   \n",
       "1    \"100 Tiger Treasures\"  [[{'cellID': -1, 'textTokens': [], 'text': 'Be...   \n",
       "\n",
       "                                        tableHeaders  tableId  \n",
       "0  [[{'cellID': -1, 'textTokens': [], 'text': 'Ye...        2  \n",
       "1  [[{'cellID': -1, 'textTokens': [], 'text': 'Aw...        4  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1a54891",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_tables = []\n",
    "weird_entities = {}\n",
    "big_tables = 0\n",
    "error_table = 0\n",
    "with tqdm(total=7066) as progress:\n",
    "    with pd.read_json(data_path, lines=True, chunksize=5000) as reader:\n",
    "        for chunk in reader:\n",
    "            for index, t in chunk.iterrows():   \n",
    "                \n",
    "                t = dataset_df.iloc[index]   \n",
    "\n",
    "                new_table = []\n",
    "                tableHeaders_cleaned = []\n",
    "                tab_id = t[\"_id\"]\n",
    "                \n",
    "                pg_Title = t[\"pgTitle\"]\n",
    "                section_Title = t[\"sectionTitle\"]\n",
    "                table_caption = t[\"tableCaption\"]    \n",
    "                tableHeaders = t[\"tableHeaders\"]    \n",
    "                table = t[\"tableData\"]                    \n",
    "                data_rows = [ r for r in table ]\n",
    "\n",
    "                headers_cells = [h for h in tableHeaders][0]\n",
    "                for c, head in enumerate(headers_cells):\n",
    "                    tableHeaders_cleaned.append([[-1,c], head['text']])\n",
    "\n",
    "                    table_data = []  \n",
    "                    row_labels = []\n",
    "                    for i, row in enumerate(table):                            \n",
    "                        for j, cell in enumerate(row):\n",
    "\n",
    "                            per_cell_labels = []\n",
    "                            cell_spans = []\n",
    "                            table_data.append([[i,j],cell[\"text\"]])\n",
    "                            if len(cell['surfaceLinks']) > 0:\n",
    "                                for k in range(len(cell['surfaceLinks'])):\n",
    "                                    subcell_i = cell['surfaceLinks'][k]\n",
    "\n",
    "                                    start_idx = subcell_i[\"offset\"]\n",
    "                                    end_idx = subcell_i[\"endOffset\"]\n",
    "                                    ent_text = subcell_i[\"surface\"]\n",
    "                                    text_lookup =  subcell_i[\"target\"][\"title\"]                           \n",
    "                            \n",
    "                                    try: \n",
    "                                        span_search = re.search(ent_text, cell[\"text\"])\n",
    "                                        if span_search:\n",
    "                                            token_start = span_search.span()[0]\n",
    "                                            token_end = span_search.span()[1]\n",
    "                                    except:\n",
    "                                        token_start = start_idx\n",
    "                                        token_end = end_idx\n",
    "                                        weird_entities[ent_text] = (text_lookup, tab_id)                                            \n",
    "\n",
    "                                    lookup_idx = np.where(nentity == text_lookup)[0]\n",
    "                                    ent_label = nlabels[lookup_idx[0]] if lookup_idx.any() else (None,None)\n",
    "\n",
    "                                    if ent_label[1]:\n",
    "                                        add_mention(ent_text, tab_id, text_lookup, dataset_entities, entity_index)\n",
    "\n",
    "                                    if ent_label[0] in labels_dict:\n",
    "                                        span_based_annotation = (i, j, token_start, token_end, labels_dict[ent_label[0]])\n",
    "                                    else:\n",
    "                                        span_based_annotation = (i, j, token_start, token_end, 0)           \n",
    "\n",
    "                                    cell_spans.append(span_based_annotation)\n",
    "\n",
    "\n",
    "                                cell[\"subcell_labels\"] = cell_spans\n",
    "                                per_cell_labels.append(cell_spans)\n",
    "                               # per_cell_labels.append(mapped)\n",
    "\n",
    "                            if len(per_cell_labels)>0:\n",
    "                                row_labels.append(per_cell_labels)                                    \n",
    "\n",
    "                new_table.append([tab_id,\n",
    "                           pg_Title,\n",
    "                           section_Title,\n",
    "                           table_caption,\n",
    "                           tableHeaders_cleaned,\n",
    "                           table_data,\n",
    "                           row_labels]\n",
    "                           )\n",
    "                all_tables.append(new_table)       \n",
    "                progress.update()              \n",
    "\n",
    "with open('../data/Wiki_TabNER_final_batch.json', 'w') as f:\n",
    "    json.dump(all_tables, f)\n",
    "\n",
    "print(len(all_tables))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "a22da268",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_entities.to_csv(\"../data/labeled_entities/dataset_entities_labeled_linked.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3a9ca81a",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs = []\n",
    "tab_ids = []\n",
    "for i in range(len(all_tables)):\n",
    "    \n",
    "    tab_ids.append(all_tables[i][0][0])\n",
    "    \n",
    "    tableHeaders = all_tables[i][0][4]\n",
    "    columns = [tableHeaders[i] for i in range(len(tableHeaders))]\n",
    "    \n",
    "    table_data = all_tables[i][0][5]\n",
    "    \n",
    "\n",
    "    row_indexes = [item[0][0] for item in table_data]\n",
    "    col_indexes = [item[0][1] for item in table_data]\n",
    "    values = [item[1] for item in table_data]\n",
    "\n",
    "    # Create a dictionary to hold the data\n",
    "    data_dict = {}\n",
    "    for row_idx, col_idx, value in zip(row_indexes, col_indexes, values):\n",
    "        if row_idx not in data_dict:\n",
    "            data_dict[row_idx] = {}\n",
    "        data_dict[row_idx][col_idx] = value\n",
    "\n",
    "    df = pd.DataFrame.from_dict(data_dict, orient='index')\n",
    "\n",
    "    # Remove the index name\n",
    "    df.index.name = None\n",
    "    dfs.append(df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "wikitabner",
   "language": "python",
   "name": "wikitabner"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
